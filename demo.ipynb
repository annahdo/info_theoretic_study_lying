{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "import importlib\n",
    "# join the path to the modules to the current working directory\n",
    "\n",
    "import utils, data, info_theory, plots\n",
    "importlib.reload(utils)\n",
    "importlib.reload(data)\n",
    "importlib.reload(info_theory)\n",
    "importlib.reload(plots)\n",
    "\n",
    "from utils import *\n",
    "from data import *\n",
    "from info_theory import *\n",
    "from plots import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'Statements1000' # load one of Statements1000, FreebaseStatements, cities\n",
    "statement_format = 'xml' # 'freeform' or 'xml'\n",
    "\n",
    "max_new_tokens = 10 # how many tokens should be generated for the statement completion\n",
    "batch_size = 64 \n",
    "\n",
    "# define which unembedding you want to use, logit lens or tuned lens \n",
    "# (tuned lens only works for models for which tuned lenses are available at https://huggingface.co/spaces/AlignmentResearch/tuned-lens/tree/main/lens)\n",
    "lens_type = \"logit_lens\" # logit_lens, tuned_lens\n",
    "\n",
    "k = 10 # how many top k tokens to consider\n",
    "\n",
    "model_name = \"HuggingFaceH4/zephyr-7b-beta\" \n",
    "# model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "# find out more about these at https://huggingface.co/docs/transformers/main/en/chat_templating\n",
    "assistant_token = \"[/INST]\\n\" if model_name.split(\"/\")[0] == \"meta-llama\" else \"\\n<|assistant|>\"\n",
    "system_token = \"[INST]\" if model_name.split(\"/\")[0] == \"meta-llama\" else \"<|system|>\"\n",
    "\n",
    "access_token = input(\"Enter your access token: \") if model_name.split(\"/\")[0] == \"meta-llama\" else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"device: {device}\")\n",
    "\n",
    "# load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, token=access_token).to(device).eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=access_token)\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_data_set(dataset_name, statement_format, assistant_token=assistant_token, system_token=system_token) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate lies/truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output tokens are saved in the data set dict\n",
    "get_overlap_truth_lies(model, tokenizer, dataset, max_new_tokens=max_new_tokens, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_examples(dataset, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the hidden states for last statement token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the modules for which we want to extract hidden states (here it's the residual stream output for each layer/decoder block)\n",
    "module_names = [f'model.layers.{i}' for i in range(model.config.num_hidden_layers)]\n",
    "num_modules = len(module_names)\n",
    "token_positions = -max_new_tokens-1 # we are tracking the last statement token\n",
    "# returns a dictionary with the hidden states of token_position (shape [len(selected_data), hidden_dim]) for each module\n",
    "dataset['hidden_states_lie'] = get_hidden_from_tokens(model, module_names, dataset['output_tokens_lie'], batch_size=batch_size, token_position=token_positions)\n",
    "dataset['hidden_states_truth'] = get_hidden_from_tokens(model, module_names, dataset['output_tokens_truth'], batch_size=batch_size, token_position=token_positions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenses = get_lens(lens_type, model.config.num_hidden_layers, model_name, hidden_size=model.config.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy and entropy delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entropy over layers\n",
    "# probability of predicted token over layers\n",
    "num_samples = len(dataset['answer_lie'])\n",
    "entropy_truth = get_entropy(model, dataset['hidden_states_truth'], lenses=lenses)\n",
    "entropy_lie = get_entropy(model, dataset['hidden_states_lie'], lenses=lenses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_median_mean(entropy_truth, entropy_lie, title=f'Entropy {dataset_name}', y_label='Entropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_rate_truth = (entropy_truth[1:]-entropy_truth[:-1]).abs()\n",
    "entropy_rate_lie = (entropy_lie[1:]-entropy_lie[:-1]).abs()\n",
    "plot_median_mean(entropy_rate_truth, entropy_rate_lie, title=f'Entropy delta {dataset_name}', y_label='Entropy delta (abs)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross entropy and cross entropy delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_truth = get_cross_entropy(model, dataset['hidden_states_truth'], lenses=lenses)\n",
    "cross_entropy_lie = get_cross_entropy(model, dataset['hidden_states_lie'], lenses=lenses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_median_mean(cross_entropy_truth, cross_entropy_lie, title=f'Cross entropy {dataset_name}', y_label='Cross entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy_rate_truth = (cross_entropy_truth[1:]-cross_entropy_truth[:-1]).abs()\n",
    "cross_entropy_rate_lie = (cross_entropy_lie[1:]-cross_entropy_lie[:-1]).abs()\n",
    "plot_median_mean(cross_entropy_rate_truth, cross_entropy_rate_lie, title=f'Cross entropy delta {dataset_name}', y_label='Cross entropy delta (abs)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KL divergence and KL divergence delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KL_truth = get_KL_divergence(model, dataset['hidden_states_truth'], lenses, mode='last')\n",
    "KL_lie = get_KL_divergence(model, dataset['hidden_states_lie'], lenses, mode='last')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_median_mean(KL_truth, KL_lie, title=f'KL divergence {dataset_name}', y_label=f'KL divergence to last layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KL_truth_rate = torch.abs(KL_truth[1:]-KL_truth[:-1])\n",
    "KL_lie_rate = torch.abs(KL_lie[1:]-KL_lie[:-1])\n",
    "plot_median_mean(KL_truth_rate, KL_lie_rate, title=f'KL divergence delta {dataset_name}', y_label=f'KL divergence delta (last layer)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability of predicted tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probability predicted token\n",
    "predicted_truth_tokens = np.array(dataset['answer_tokens_truth'])[:,0] # the target token is the first answer token\n",
    "prob_truth = get_probability(model, dataset['hidden_states_truth'], lenses, target_token=predicted_truth_tokens)\n",
    "predicted_lie_tokens = np.array(dataset['answer_tokens_lie'])[:,0] # the target token is the first answer token\n",
    "prob_lie = get_probability(model, dataset['hidden_states_lie'], lenses, target_token=predicted_lie_tokens)\n",
    "\n",
    "# probability truth token\n",
    "prob_lie_track_truth_token = get_probability(model, dataset['hidden_states_lie'], lenses, target_token=predicted_truth_tokens)\n",
    "\n",
    "# probability lie token\n",
    "prob_truth_track_lie_token = get_probability(model, dataset['hidden_states_truth'], lenses, target_token=predicted_lie_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_median_mean(prob_truth, prob_lie, title=f'Token probability {dataset_name}', y_label='Probability of predicted token')\n",
    "plot_median_mean(prob_truth, prob_lie_track_truth_token, title=f'Truth token probability {dataset_name}', y_label='Probability of truth token')\n",
    "plot_median_mean(prob_truth_track_lie_token, prob_lie, title=f'Lie token probability {dataset_name}', y_label='Probability of lie token')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# track probability of top k tokens\n",
    "top_k_prob_truth = torch.zeros((k,)+prob_truth.shape)\n",
    "top_k_prob_lie = torch.zeros((k,)+prob_lie.shape)\n",
    "\n",
    "top_k_truth_tokens = torch.topk(unembed(model, dataset['hidden_states_truth'][-1]), k, dim=-1)\n",
    "top_k_lie_tokens = torch.topk(unembed(model, dataset['hidden_states_lie'][-1]), k, dim=-1)\n",
    "\n",
    "for i in range(k):\n",
    "    top_k_prob_truth[i] = get_probability(model, dataset['hidden_states_truth'], lenses, target_token=top_k_truth_tokens.indices[:,i])\n",
    "    top_k_prob_lie[i] = get_probability(model, dataset['hidden_states_lie'], lenses, target_token=top_k_lie_tokens.indices[:,i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_layers = [15, 20, 25, 28, 30, 31]\n",
    "prob_truth_means, prob_truth_medians = top_k_prob_truth.mean(dim=-1), top_k_prob_truth.median(dim=-1).values\n",
    "plot_h_bar(top_k_prob_truth, top_k_prob_lie, selected_layers, title=f'Probability of top {k} output tokens {dataset_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_matching_truth = find_matching_tokens(tokenizer, k, top_k_truth_tokens.indices)\n",
    "top_k_matching_lie = find_matching_tokens(tokenizer, k, top_k_lie_tokens.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum over all prob that match tokens that are considered\n",
    "new_shape = (k, 1, num_samples)\n",
    "prob_truth_sums = (top_k_prob_truth*torch.transpose(top_k_matching_truth, 0,1).unsqueeze(1)).sum(dim=0)\n",
    "prob_lie_sums = (top_k_prob_lie*torch.transpose(top_k_matching_lie, 0,1).unsqueeze(1)).sum(dim=0)\n",
    "plot_median_mean(prob_truth_sums, prob_lie_sums, title=f'Token probability {dataset_name} summed', y_label='Probability of predicted token')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
